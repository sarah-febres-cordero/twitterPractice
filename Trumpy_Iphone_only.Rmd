---
title: "trump_practice"
author: "SFC"
date: "3/11/2019"
output: html_document
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE,
                      dev = "svg")
library(ggplot2)
theme_set(theme_bw())
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r libraries}
library(dplyr)
library(purrr)
library(twitteR)
library(readr)
library(tidytext)
library(RCurl); packageVersion("RCurl")
library(tidyverse); packageVersion("tidyverse")
library(stringr); packageVersion("stringr")
library(rvest); packageVersion("rvest")
library(methods); packageVersion("methods")
library(tm); packageVersion("tm")
library(wordcloud); packageVersion("wordcloud")
library(RColorBrewer); packageVersion("RColorBrewer")
```

## Including Plots



```{r pressure, echo=FALSE}

#to get your consumerKey and consumerSecret see the twitteR documentation for instructions
consumer_key <- 'yvr6juDS7eXLupzMYX17FQY2I'
consumer_secret <-    'rrTarC1GSt0PvrkHf56bZ5O4wFfpLWyzUqyTOtluJPOKRw7L0f'
access_token <- '820657692708577281-TF9pAeAHtZz6UGpiToXHf7fPnbtY7hl'
access_secret <- 'jxsGYPuEExHtuC4tV8pnQ8avUCWjWmnVvqZ8v0yuj1DzD'
setup_twitter_oauth(consumer_key,
                    consumer_secret,
                    access_token,
                    access_secret)
```
## Pull tweets, my API only alows for 98

```{r tweets, dependson = "trump_tweets"}
trump_tweets <- userTimeline("realDonaldTrump", n = 3200)
```
## With Data Frame
```{r tweets, dependson = "trump_tweets_df"}
trump_tweets_df <- tbl_df(map_df(trump_tweets, as.data.frame))
```
## Clean up Data
```{r tweets, dependson = "trump_tweets_df"}
library(tidyr)
tweets <- trump_tweets_df %>%
  select(id, statusSource, text, created) %>%
  extract(statusSource, "source", "Twitter for (.*?)<") %>%
  filter(source %in% c("iPhone", "Android"))
```
## Time of day 
```{r dependson = "tweets"}
library(lubridate)
library(scales)
tweets %>%
  count(source, hour = hour(with_tz(created, "EST"))) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")
```
##
```{r dependson = "tweets", echo = FALSE}
library(stringr)
tweets %>%
  count(source,
        quoted = ifelse(str_detect(text, '^"'), "Quoted", "Not quoted")) %>%
  ggplot(aes(source, n, fill = quoted)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "") +
  ggtitle('Whether tweets start with a quotation mark (")')

```
##
```{r, dependson = "tweets"}
tweet_picture_counts <- tweets %>%
  filter(!str_detect(text, '^"')) %>%
  count(source,
        picture = ifelse(str_detect(text, "t.co"),
                         "Picture/link", "No picture/link"))
ggplot(tweet_picture_counts, aes(source, n, fill = picture)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "")

```
###
```{r echo = FALSE}
spr <- tweet_picture_counts %>%
  spread(source, n) %>%
  mutate_each(funs(. / sum(.)), iPhone)
rr <- spr$iPhone[2] 
```
## Trump no longer tweets from unsecured Android
```{r tweet_words, dependson = "tweets"}
library(tidytext)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
tweet_words
```
## Plot commonly used words
```{r tweet_words_plot, dependson = "tweet_words", fig.height = 6, fig.width = 8, echo = FALSE}
tweet_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip()
```
## Sentiment analysis
```{r nrc}
nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  dplyr::select(word, sentiment)
nrc
```
## 
```{r by_source_sentiment, dependson = c("nrc", "tweet_words")}
sources <- tweet_words %>%
  group_by(source) %>%
  mutate(total_words = n()) %>%
  ungroup() %>%
  distinct(id, source, total_words)
by_source_sentiment <- tweet_words %>%
  inner_join(nrc, by = "word") %>%
  count(sentiment, id) %>%
  ungroup() %>%
  complete(sentiment, id, fill = list(n = 0)) %>%
  inner_join(sources) %>%
  group_by(source, sentiment, total_words) %>%
  summarize(words = sum(n)) %>%
  ungroup()
head(by_source_sentiment)
```
##
```{r}
library(broom)
sentiment_differences <- by_source_sentiment %>%
  group_by(sentiment) %>%
  do(tidy(poisson.test(.$words, .$total_words)))
sentiment_differences
```
## Visualize with 95% confidence interval
```{r dependson = "sentiment_differences", echo = FALSE, fig.height = 8, fig.width = 8}
library(scales)
sentiment_differences %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, estimate)) %>%
  mutate_each(funs(. - 1), estimate, conf.low, conf.high) %>%
  ggplot(aes(estimate, sentiment)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  scale_x_continuous(labels = percent_format()) +
  labs(x = "iPhone",
       y = "Sentiment")
```
## Word Cloud
```{r}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
```
## text file import
```{r}

clean.text <- function(tweet_words)
{
tweet_words = gsub("(RT|via)((?:/b/W*@/w+)+)","", tweet_words)
tweet_words = gsub("@/w+", "", tweet_words)
tweet_words = gsub("[[:punct:]]", "", tweet_words)
tweet_words = gsub("[[:digit:]]", "", tweet_words)
tweet_words = gsub("http/w+", "", tweet_words)
tweet_words = gsub("[ t]{2,}", "", tweet_words)
tweet_words = gsub("^/s+|/s+$", "", tweet_words)
tweet_words = gsub("amp", "", tweet_words)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
 
tweet_words = sapply(tweet_words, try.tolower)
tweet_words = tweet_words[tweet_words != ""]
names(tweet_words) = NULL
return(tweet_words)
}
 
getSentiment <- function (text, key){
 
text <- URLencode(text);
 
#save all the spaces, then get rid of the weird characters that break the API, then convert back the URL-encoded spaces.
text <- str_replace_all(text, "%20", " ");
text <- str_replace_all(text, "%/d/d", "");
text <- str_replace_all(text, " ", "%20");
 
if (str_length(text) > 736){
text <- substr(text, 0, 735);
}
##########################################
 

 
# get mood probability
sentiment = js$output$result
 
###################################
return(list(sentiment=sentiment))
}
```
## Get text
```{r}
# clean text
tweet_clean = clean.text(tweet_words)
tweet_num = length(tweet_clean)
```
## create data frame
```{r}
tweet_df = data.frame(text=tweet_clean, sentiment=rep("", tweet_num),stringsAsFactors=FALSE)
```
##
```{r}
# apply function getSentiment
sentiment = rep(0, tweet_num)
for (i in 1:tweet_num)

 
# delete rows with no sentiment
tweet_df <- tweet_df[tweet_df$sentiment!="",]
```
```{r}
#separate text by sentiment
sents = levels(factor(tweet_df$sentiment))
```
## get labels
```{r}
labels <- lapply(sents, function(x) paste(x,format(round((length((tweet_df[tweet_df$sentiment ==x,])$text)/length(tweet_df$sentiment)*100),2),nsmall=2),"%"))
```
##
```{r}
nemo = length(sents)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = tweet_df[tweet_df$sentiment == sents[i],]$text
 
emo.docs[i] = paste(tmp,collapse=" ")
}
```
##
```{r}
# remove stopwords
emo.docs = removeWords(emo.docs, stopwords("german"))
emo.docs = removeWords(emo.docs, stopwords("english"))
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = labels
 
library(ggplot2)
library(Dark)
library(comparison)
# comparison word cloud

wordcloud(VCorpus(VectorSource(tweet_df$emo.docs)), max.words = 30, scale = c(4, 1), colors = topo.colors(n=30), random.color = TRUE)
```
